{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consecutive-coral",
   "metadata": {},
   "source": [
    "Below is a solution to [LeetCode 1236 - Web Crawler](https://leetcode.com/problems/web-crawler/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "speaking-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This is HtmlParser's API interface.\n",
    "# You should not implement it, or speculate about its implementation\n",
    "# \"\"\"\n",
    "# class HtmlParser(object):\n",
    "#    def getUrls(self, url):\n",
    "#        \"\"\"\n",
    "#        :type url: str\n",
    "#        :rtype List[str]\n",
    "#        \"\"\"\n",
    "from typing import List\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_hostname(url: str):\n",
    "    url = urlparse(url).hostname\n",
    "    if not url:\n",
    "        return\n",
    "    found_dot = False\n",
    "    for i, c in enumerate(reversed(url)):\n",
    "        if c != '.':\n",
    "            continue\n",
    "        if not found_dot:\n",
    "            found_dot = True\n",
    "        else:\n",
    "            return url[len(url)-i:]\n",
    "    return url\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def crawl(self, start_url: str, html_parser: 'HtmlParser') -> List[str]:\n",
    "        hostname = get_hostname(start_url)\n",
    "        visited = set()\n",
    "        stack = [start_url]\n",
    "        while stack:\n",
    "            curr_url = stack.pop()\n",
    "            if get_hostname(curr_url) != hostname:\n",
    "                continue\n",
    "            visited.add(curr_url)\n",
    "            for child in html_parser.getUrls(curr_url):\n",
    "                if child not in visited:\n",
    "                    stack.append(child)\n",
    "        return list(visited)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-front",
   "metadata": {},
   "source": [
    "## Concurrency with `concurrent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-donna",
   "metadata": {},
   "source": [
    "Let's now expand this web crawler to run asynchronously. Firstly, here is Python pseudocode for how we would implement a multithreaded if we could magically use threads. \n",
    "```python3\n",
    "def crawl_thread(task_pool, htmlParser, new_url, visited_urls):\n",
    "    visited_urls.lock()\n",
    "    if new_url in visited_urls:\n",
    "        return\n",
    "    visited_urls.add(new_url)\n",
    "    visited_urls.unlock()\n",
    "    \n",
    "    children = htmlParser.getUrls()\n",
    "    for child in children:\n",
    "        if child not in visited:\n",
    "            task_pool.lock()\n",
    "            new_task = crawl_thread(task_pool, htmlParser, new_url, visited_urls)\n",
    "            task_pool.add(new_task)\n",
    "            task_pool.unlock()\n",
    "            new_task.start()\n",
    "\n",
    "\n",
    "def crawl(starting_url):\n",
    "    task_pool = [crawl_thread(task_pool, htmlParser, new_url, visited_urls)]\n",
    "    visited_urls = []\n",
    "    task_pool.run()\n",
    "    return visited_urls\n",
    "\n",
    "```\n",
    "Python has some challenges around writing concurrent code because of the Global Interpreter Lock (GIL) and non-thread-safe memory model so there's a couple workaround. One workaround is the `concurrent` library. We can use this to implement a solution to [LeetCode 1242 - Web Crawler Multithreaded](https://leetcode.com/problems/web-crawler-multithreaded/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_hostname(url: str):\n",
    "    url = urlparse(url).hostname\n",
    "    if not url:\n",
    "        return\n",
    "    found_dot = False\n",
    "    for i, c in enumerate(reversed(url)):\n",
    "        if c != '.':\n",
    "            continue\n",
    "        if not found_dot:\n",
    "            found_dot = True\n",
    "        else:\n",
    "            return url[len(url)-i:]\n",
    "    return url\n",
    "\n",
    "def crawl_thread(executor, current_url, base_url, html_parser, visited_urls, futures):\n",
    "    visited_urls.add(current_url)\n",
    "    children = html_parser.getUrls(current_url)\n",
    "    for child in children:\n",
    "        if (child not in visited_urls) and (get_hostname(child) == base_url):\n",
    "            future = executor.submit(crawl_thread, executor, child, base_url, html_parser, visited_urls, futures)\n",
    "            futures.add(future)\n",
    "            if future.result():\n",
    "                raise RuntimeError(f\"{future.result()}\")\n",
    "    \n",
    "class Solution:\n",
    "    def crawl(self, start_url, html_parser):\n",
    "        base_url = get_hostname(start_url)\n",
    "        visited_urls = set([start_url]) \n",
    "        futures = set()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "            initial_future = executor.submit(crawl_thread, executor, start_url, base_url, html_parser, visited_urls, futures)\n",
    "            futures.add(initial_future)\n",
    "            for future in as_completed(futures):\n",
    "                # Spinlock until all threads complete\n",
    "                pass\n",
    "        \n",
    "        return list(visited_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-personality",
   "metadata": {},
   "source": [
    "## Concurrency with `asyncio`\n",
    "Another option is the `asyncio` library, which is great for i/o bound activity. Here's an example of the crawler implemented with `asyncio` coroutines (this will not actually pass the LeetCode test battery, because it TLEs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from urllib.parse import urlparse\n",
    "import asyncio\n",
    "\n",
    "def get_hostname(url: str):\n",
    "    url = urlparse(url).hostname\n",
    "    if not url:\n",
    "        return\n",
    "    found_dot = False\n",
    "    for i, c in enumerate(reversed(url)):\n",
    "        if c != '.':\n",
    "            continue\n",
    "        if not found_dot:\n",
    "            found_dot = True\n",
    "        else:\n",
    "            return url[len(url)-i:]\n",
    "    return url\n",
    "\n",
    "\n",
    "async def crawl_task(base_host, url, html_parser, visited_urls, semaphore):\n",
    "    visited_urls.add(url)\n",
    "    \n",
    "    tasks = []\n",
    "    async with semaphore:\n",
    "        children = html_parser.getUrls(url)\n",
    "    \n",
    "    for child in children:\n",
    "        if child not in visited_urls and get_hostname(child) == base_host:\n",
    "            task = asyncio.create_task(crawl_task(base_host, child, html_parser, visited_urls, semaphore))\n",
    "            tasks.append(task)\n",
    "    await asyncio.gather(*tasks)\n",
    "            \n",
    "                                       \n",
    "class Solution:\n",
    "    def crawl(self, start_url: str, html_parser: 'HtmlParser') -> List[str]:\n",
    "        visited_urls = set()\n",
    "        semaphore = asyncio.Semaphore(60)\n",
    "        asyncio.run(crawl_task(get_hostname(start_url), start_url, html_parser, visited_urls, semaphore))\n",
    "        return list(visited_urls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
